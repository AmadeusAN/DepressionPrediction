{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:18:54.716163600Z",
     "start_time": "2024-07-26T07:18:50.857438200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import requests\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:18:54.730837200Z",
     "start_time": "2024-07-26T07:18:54.718185Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio_path = r\"/public1/cjh/workspace/DepressionPrediction/dataset/EATD-Corpus/train/2/positive.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:18:55.479289600Z",
     "start_time": "2024-07-26T07:18:55.425868200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 65376])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "# if waveform.shape[0] > 1:\n",
    "#     waveform = waveform[0]\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65376])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform = torch.unsqueeze(waveform,dim = 0)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to pad audio to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_audio(signal, target_length):\n",
    "    B,L = signal.shape\n",
    "    # 计算需要填充的长度\n",
    "    padding_length = target_length - L\n",
    "    if padding_length > 0:\n",
    "        # 使用零填充\n",
    "        signal = torch.cat([signal, torch.zeros(B,padding_length)], dim=-1)\n",
    "    return signal\n",
    "\n",
    "target_length = 300000  # 例如，填充到 1 秒的长度，假设采样率为 16 kHz\n",
    "\n",
    "padded_signal = pad_audio(waveform, target_length)\n",
    "padded_signal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to use native pytorch api to do STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public1/cjh/anaconda3/envs/depression/lib/python3.10/site-packages/torch/functional.py:666: UserWarning: A window was not provided. A rectangular window will be applied,which is known to cause spectral leakage. Other windows such as torch.hann_window or torch.hamming_window can are recommended to reduce spectral leakage.To suppress this warning and use a rectangular window, explicitly set `window=torch.ones(n_fft, device=<device>)`. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538440907/work/aten/src/ATen/native/SpectralOps.cpp:836.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1025, 586])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_length = 2048  # 窗口长度\n",
    "hop_length = 512  # 窗口滑动步长\n",
    "result = torch.stft(padded_signal,n_fft=window_length,hop_length=hop_length, return_complex=True)\n",
    "result.shape    # (N,T) N-#freuqencies,T-#frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.complex64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8640, -0.6591, -0.3039,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3644, -0.3990, -0.3288,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.7611, -0.1673, -0.2878,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0059,  0.0148, -0.0641,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0047,  0.0153, -0.0638,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0051,  0.0150, -0.0650,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-6.2585e-07,  2.3596e-01, -5.1774e-02,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-8.0466e-07, -4.4250e-03, -2.3048e-01,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 8.9407e-08,  2.7411e-05,  2.9087e-05,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-4.9174e-07,  8.0549e-04,  1.5150e-04,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE-attention mechanism\n",
    "最后将要压缩频谱特征，因此在时间特征上加注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1025, 586])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.stack([result.real,result.imag],dim = 1)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input shape = [2, 1025, 586], which means [C, N, T]\n",
    "N : frequencies \n",
    "T : #frames\n",
    "\n",
    "try to attach a se-attentive module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1025, 586])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = input_tensor\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1025, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequeeze = torch.nn.AdaptiveAvgPool1d(output_size=1)\n",
    "input_se = sequeeze(input)\n",
    "input_se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_se_real = input_se[0]\n",
    "input_se_imag = input_se[1]\n",
    "input_se_real = torch.squeeze(input_se_real,dim=-1)\n",
    "input_se_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract = nn.Sequential(\n",
    "            nn.Linear(1025,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048,1025),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "s = extract(input_se_real)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025, 586])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_real = input[0]\n",
    "input_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.unsqueeze(s,dim = -1)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025, 586])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_real_enhance = input_real * s\n",
    "input_real_enhance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_module(nn.Module):\n",
    "    def __init__(self, in_channel:int = 1025, k:int = 2048):\n",
    "        super(SE_module,self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.k = k\n",
    "        self.sequeeze = torch.nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.extract = nn.Sequential(\n",
    "            nn.Linear(self.in_channel,self.k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.k,self.in_channel),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x:shape: (batch_size,C, N, T)\n",
    "        '''\n",
    "        \n",
    "        u_r = self.sequeeze(x[:,0,:,:])    # b,1025,1\n",
    "        u_i = self.sequeeze(x[:,1,:,:])\n",
    "        u_r = torch.squeeze(input=u_r, dim=-1)  # b,1025\n",
    "        u_i = torch.squeeze(input=u_i, dim=-1)\n",
    "        a_r = torch.unsqueeze(self.extract(u_r),dim = -1) # b,1025,1\n",
    "        a_i = torch.unsqueeze(self.extract(u_i),dim = -1)\n",
    "\n",
    "        x_r_enhance = x[:,0,:,:] * a_r\n",
    "        x_i_enhance = x[:,1,:,:] * a_i\n",
    "\n",
    "        output = torch.stack([x_r_enhance,x_i_enhance],dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1025, 586])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = SE_module()\n",
    "dummy_y = se.forward(torch.randn(1, 2, 1025, 586))\n",
    "dummy_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrink module\n",
    "input： [B,2,N,T]\n",
    "output: [B,V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1025, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shrink_axis = 586\n",
    "a = nn.Conv2d(in_channels=2,out_channels=1,kernel_size=(1,shrink_axis//3))(dummy_y)\n",
    "b = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,shrink_axis//2))(a)\n",
    "c = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,b.shape[-1]))(b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shrink(nn.Module):\n",
    "    def __init__(self,shrink_size:int = 586):\n",
    "        super(Shrink, self).__init__()\n",
    "        self.shrink_size = shrink_size\n",
    "        self.inner_net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2,out_channels=1,kernel_size=(1,shrink_axis//3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,shrink_axis//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,b.shape[-1]))\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.squeeze(self.inner_net(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shrink = Shrink(shrink_size=586)\n",
    "y = shrink(dummy_y)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
