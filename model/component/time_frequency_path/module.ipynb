{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:18:54.716163600Z",
     "start_time": "2024-07-26T07:18:50.857438200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import requests\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:18:54.730837200Z",
     "start_time": "2024-07-26T07:18:54.718185Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio_path = r\"/public1/cjh/workspace/DepressionPrediction/dataset/EATD-Corpus/train/2/positive.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T07:18:55.479289600Z",
     "start_time": "2024-07-26T07:18:55.425868200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65376])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform[0]\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65376])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform = torch.unsqueeze(waveform,dim = 0)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to pad audio to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_audio(signal, target_length):\n",
    "    B,L = signal.shape\n",
    "    # 计算需要填充的长度\n",
    "    padding_length = target_length - L\n",
    "    if padding_length > 0:\n",
    "        # 使用零填充\n",
    "        signal = torch.cat([signal, torch.zeros(B,padding_length)], dim=-1)\n",
    "    return signal\n",
    "\n",
    "target_length = 300000  # 例如，填充到 1 秒的长度，假设采样率为 16 kHz\n",
    "\n",
    "padded_signal = pad_audio(waveform, target_length)\n",
    "padded_signal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to use native pytorch api to do STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1025, 1025])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_length = 2048  # 窗口长度\n",
    "hop_length = waveform.shape[-1]  // 1024  # 窗口滑动步长\n",
    "result = torch.stft(waveform,n_fft=window_length,hop_length=hop_length, return_complex=True)[:,:,0:1025]\n",
    "result.shape    # (N,T) N-#freuqencies,T-#frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.complex64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8640, -1.1376, -0.7755,  ...,  0.5400,  0.7155,  0.3819],\n",
       "         [-0.3644, -0.6454, -0.3040,  ...,  0.4600,  0.6292,  0.2743],\n",
       "         [-0.7611, -1.0355, -0.6692,  ...,  0.5474,  0.6951,  0.2645],\n",
       "         ...,\n",
       "         [ 0.0059, -0.0452,  0.1658,  ...,  0.0248, -0.0226, -0.0128],\n",
       "         [ 0.0047, -0.0440,  0.1647,  ...,  0.0251, -0.0231, -0.0119],\n",
       "         [ 0.0051, -0.0444,  0.1651,  ...,  0.0249, -0.0230, -0.0117]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-6.2585e-07,  9.8414e-02,  1.7925e-01,  ...,  1.8681e-03,\n",
       "           8.8565e-02,  1.0981e-01],\n",
       "         [-8.0466e-07,  4.5920e-02,  6.1775e-02,  ..., -5.8796e-03,\n",
       "           1.9922e-01,  2.5276e-01],\n",
       "         ...,\n",
       "         [ 8.9407e-08,  3.2350e-04, -8.1351e-04,  ..., -3.8803e-04,\n",
       "           1.4542e-03, -2.2108e-03],\n",
       "         [-4.9174e-07, -8.0407e-05,  6.0692e-05,  ..., -3.7193e-04,\n",
       "           9.7570e-04, -1.4924e-03],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE-attention mechanism\n",
    "最后将要压缩频谱特征，因此在时间特征上加注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1025, 1025])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.stack([result.real,result.imag],dim = 1)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input shape = [2, 1025, 586], which means [C, N, T]\n",
    "N : frequencies \n",
    "T : #frames\n",
    "\n",
    "try to attach a se-attentive module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1025, 1025])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = input_tensor\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2 to 3 dimensions, but got 4-dimensional tensor for argument #1 'self' (while checking arguments for adaptive_avg_pool1d)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sequeeze \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool1d(output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m input_se \u001b[38;5;241m=\u001b[39m \u001b[43msequeeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m input_se\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/depression/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/depression/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/depression/lib/python3.10/site-packages/torch/nn/modules/pooling.py:1228\u001b[0m, in \u001b[0;36mAdaptiveAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2 to 3 dimensions, but got 4-dimensional tensor for argument #1 'self' (while checking arguments for adaptive_avg_pool1d)"
     ]
    }
   ],
   "source": [
    "sequeeze = torch.nn.AdaptiveAvgPool1d(output_size=1)\n",
    "input_se = sequeeze(input)\n",
    "input_se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_se_real = input_se[0]\n",
    "input_se_imag = input_se[1]\n",
    "input_se_real = torch.squeeze(input_se_real,dim=-1)\n",
    "input_se_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract = nn.Sequential(\n",
    "            nn.Linear(1025,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048,1025),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "s = extract(input_se_real)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025, 586])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_real = input[0]\n",
    "input_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.unsqueeze(s,dim = -1)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025, 586])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_real_enhance = input_real * s\n",
    "input_real_enhance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_module(nn.Module):\n",
    "    def __init__(self, in_channel:int = 1025, k:int = 2048):\n",
    "        super(SE_module,self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.k = k\n",
    "        self.sequeeze = torch.nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.extract = nn.Sequential(\n",
    "            nn.Linear(self.in_channel,self.k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.k,self.in_channel),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x:shape: (batch_size,C, N, T)\n",
    "        '''\n",
    "        \n",
    "        u_r = self.sequeeze(x[:,0,:,:])    # b,1025,1\n",
    "        u_i = self.sequeeze(x[:,1,:,:])\n",
    "        u_r = torch.squeeze(input=u_r, dim=-1)  # b,1025\n",
    "        u_i = torch.squeeze(input=u_i, dim=-1)\n",
    "        a_r = torch.unsqueeze(self.extract(u_r),dim = -1) # b,1025,1\n",
    "        a_i = torch.unsqueeze(self.extract(u_i),dim = -1)\n",
    "\n",
    "        x_r_enhance = x[:,0,:,:] * a_r\n",
    "        x_i_enhance = x[:,1,:,:] * a_i\n",
    "\n",
    "        output = torch.stack([x_r_enhance,x_i_enhance],dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1025, 586])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se = SE_module()\n",
    "dummy_y = se.forward(torch.randn(1, 2, 1025, 586))\n",
    "dummy_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrink module\n",
    "input： [B,2,N,T]\n",
    "output: [B,V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1025, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shrink_axis = 586\n",
    "a = nn.Conv2d(in_channels=2,out_channels=1,kernel_size=(1,shrink_axis//3))(dummy_y)\n",
    "b = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,shrink_axis//2))(a)\n",
    "c = nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,b.shape[-1]))(b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shrink(nn.Module):\n",
    "    def __init__(self,shrink_size:int = 586):\n",
    "        super(Shrink, self).__init__()\n",
    "        self.shrink_size = shrink_size\n",
    "        self.inner_net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2,out_channels=1,kernel_size=(1,shrink_axis//3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,shrink_axis//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(1,b.shape[-1]))\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.squeeze(self.inner_net(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1025])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shrink = Shrink(shrink_size=586)\n",
    "y = shrink(dummy_y)\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
